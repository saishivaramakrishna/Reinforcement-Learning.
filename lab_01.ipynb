{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saishivaramakrishna/Reinforcement-Learning./blob/main/lab_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE0BhKUiqKTZ",
        "outputId": "6a86cfe7-60b2-4000-8c78-8071617e8c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "游대 Value Iteration:\n",
            "Policy: {'s0': 'a0', 's1': 'a0', 's2': 'a0'}\n",
            "Values: {'s0': 21.575091698945428, 's1': 18.417582529050886, 's2': 0}\n",
            "\n",
            "游대 Policy Iteration:\n",
            "Policy: {'s0': 'a0', 's1': 'a0', 's2': 'a0'}\n",
            "Values: {'s0': 21.575325291175456, 's1': 18.41779276205791, 's2': 0}\n"
          ]
        }
      ],
      "source": [
        "# Simple MDP definition\n",
        "states = ['s0', 's1', 's2']\n",
        "actions = ['a0', 'a1']\n",
        "terminal_state = 's2'\n",
        "\n",
        "# Transition model: P[state][action] = list of (prob, next_state, reward)\n",
        "P = {\n",
        "    's0': {\n",
        "        'a0': [(1.0, 's1', 5)],\n",
        "        'a1': [(1.0, 's2', 10)]\n",
        "    },\n",
        "    's1': {\n",
        "        'a0': [(1.0, 's0', -1)],\n",
        "        'a1': [(1.0, 's2', 2)]\n",
        "    },\n",
        "    's2': {\n",
        "        'a0': [(1.0, 's2', 0)],\n",
        "        'a1': [(1.0, 's2', 0)]\n",
        "    }\n",
        "}\n",
        "\n",
        "gamma = 0.9\n",
        "theta = 0.001\n",
        "\n",
        "# ------------------ Value Iteration ------------------\n",
        "def value_iteration(P, states, actions, gamma=0.9, theta=0.001):\n",
        "    V = {s: 0 for s in states}\n",
        "    policy = {s: actions[0] for s in states}\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in states:\n",
        "            if s == terminal_state:\n",
        "                continue\n",
        "            action_values = []\n",
        "            for a in actions:\n",
        "                value = sum([prob * (reward + gamma * V[next_state])\n",
        "                             for prob, next_state, reward in P[s][a]])\n",
        "                action_values.append((value, a))\n",
        "            max_value, best_action = max(action_values)\n",
        "            delta = max(delta, abs(V[s] - max_value))\n",
        "            V[s] = max_value\n",
        "            policy[s] = best_action\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return policy, V\n",
        "\n",
        "# ------------------ Policy Iteration ------------------\n",
        "def policy_evaluation(policy, P, states, gamma=0.9, theta=0.001):\n",
        "    V = {s: 0 for s in states}\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in states:\n",
        "            if s == terminal_state:\n",
        "                continue\n",
        "            a = policy[s]\n",
        "            v = sum([prob * (reward + gamma * V[next_state])\n",
        "                     for prob, next_state, reward in P[s][a]])\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "            V[s] = v\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "def policy_iteration(P, states, actions, gamma=0.9, theta=0.001):\n",
        "    policy = {s: actions[0] for s in states}\n",
        "    while True:\n",
        "        V = policy_evaluation(policy, P, states, gamma, theta)\n",
        "        policy_stable = True\n",
        "        for s in states:\n",
        "            if s == terminal_state:\n",
        "                continue\n",
        "            old_action = policy[s]\n",
        "            action_values = {\n",
        "                a: sum([prob * (reward + gamma * V[next_state])\n",
        "                        for prob, next_state, reward in P[s][a]])\n",
        "                for a in actions\n",
        "            }\n",
        "            best_action = max(action_values, key=action_values.get)\n",
        "            policy[s] = best_action\n",
        "            if old_action != best_action:\n",
        "                policy_stable = False\n",
        "        if policy_stable:\n",
        "            break\n",
        "    return policy, V\n",
        "\n",
        "# ------------------ Run Both Algorithms ------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"游대 Value Iteration:\")\n",
        "    vi_policy, vi_values = value_iteration(P, states, actions)\n",
        "    print(\"Policy:\", vi_policy)\n",
        "    print(\"Values:\", vi_values)\n",
        "\n",
        "    print(\"\\n游대 Policy Iteration:\")\n",
        "    pi_policy, pi_values = policy_iteration(P, states, actions)\n",
        "    print(\"Policy:\", pi_policy)\n",
        "    print(\"Values:\", pi_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61fc1dc5"
      },
      "source": [
        "# Define the states of the MDP. In this case, representing different levels of solar power availability.\n",
        "# Increased to 5 states\n",
        "states = [\"Very Low Solar\", \"Low Solar\", \"Medium Solar\", \"High Solar\", \"Very High Solar\"]\n",
        "\n",
        "# Define the actions the agent can take in each state.\n",
        "actions = [\"Activate Generator\", \"Do Not Activate Generator\"]\n",
        "\n",
        "# Define transition probabilities (conceptual example).\n",
        "# This is a dictionary where:\n",
        "# - The first level key is the current state (s).\n",
        "# - The second level key is the action taken (a).\n",
        "# - The third level key is the next state (s').\n",
        "# - The value is the probability of transitioning from s to s' given action a (P(s'|s,a)).\n",
        "# These probabilities would ideally be derived from historical data or a system model.\n",
        "# *** NOTE: You will need to fill in the specific probabilities for the new states\n",
        "# and adjust existing ones as needed for your 5-state model. ***\n",
        "transition_probabilities = {\n",
        "    \"Very Low Solar\": {\n",
        "        \"Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.6, \"Low Solar\": 0.3, \"Medium Solar\": 0.1, \"High Solar\": 0.0, \"Very High Solar\": 0.0\n",
        "        },\n",
        "        \"Do Not Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.7, \"Low Solar\": 0.2, \"Medium Solar\": 0.1, \"High Solar\": 0.0, \"Very High Solar\": 0.0\n",
        "        }\n",
        "    },\n",
        "    \"Low Solar\": {\n",
        "        \"Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.1, \"Low Solar\": 0.5, \"Medium Solar\": 0.3, \"High Solar\": 0.1, \"Very High Solar\": 0.0\n",
        "        },\n",
        "        \"Do Not Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.1, \"Low Solar\": 0.6, \"Medium Solar\": 0.2, \"High Solar\": 0.1, \"Very High Solar\": 0.0\n",
        "        }\n",
        "    },\n",
        "    \"Medium Solar\": {\n",
        "        \"Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.0, \"Low Solar\": 0.2, \"Medium Solar\": 0.6, \"High Solar\": 0.2, \"Very High Solar\": 0.0\n",
        "        },\n",
        "        \"Do Not Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.0, \"Low Solar\": 0.3, \"Medium Solar\": 0.5, \"High Solar\": 0.2, \"Very High Solar\": 0.0\n",
        "        }\n",
        "    },\n",
        "    \"High Solar\": {\n",
        "        \"Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.0, \"Low Solar\": 0.1, \"Medium Solar\": 0.3, \"High Solar\": 0.5, \"Very High Solar\": 0.1\n",
        "        },\n",
        "        \"Do Not Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.0, \"Low Solar\": 0.05, \"Medium Solar\": 0.15, \"High Solar\": 0.6, \"Very High Solar\": 0.2\n",
        "        }\n",
        "    },\n",
        "    \"Very High Solar\": {\n",
        "        \"Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.0, \"Low Solar\": 0.0, \"Medium Solar\": 0.1, \"High Solar\": 0.3, \"Very High Solar\": 0.6\n",
        "        },\n",
        "        \"Do Not Activate Generator\": {\n",
        "            \"Very Low Solar\": 0.0, \"Low Solar\": 0.0, \"Medium Solar\": 0.05, \"High Solar\": 0.15, \"Very High Solar\": 0.8\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define the reward function (conceptual example).\n",
        "# This is a dictionary where:\n",
        "# - The first level key is the current state (s).\n",
        "# - The second level key is the action taken (a).\n",
        "# - The third level key is the next state (s').\n",
        "# - The value is the immediate reward received when taking action a in state s and transitioning to state s' (R(s,a,s')).\n",
        "# The rewards are defined to incentivize meeting demand efficiently.\n",
        "# Assuming High Solar and Very High Solar meet demand alone (+10 reward when generator is off).\n",
        "# Medium Solar might sometimes meet demand (+5 reward when generator is off), Low/Very Low do not (-5 penalty when generator is off).\n",
        "# Activating the generator always meets demand (+5 reward), regardless of solar state.\n",
        "# *** NOTE: You will need to fill in the specific reward values for the new states\n",
        "# and adjust existing ones as needed for your 5-state model. ***\n",
        "reward_function = {\n",
        "    \"Very Low Solar\": {\n",
        "        \"Activate Generator\": {s_prime: 5 for s_prime in states}, # Generator active, meets demand\n",
        "        \"Do Not Activate Generator\": {s_prime: -5 for s_prime in states} # No generator, likely unmet demand\n",
        "    },\n",
        "    \"Low Solar\": {\n",
        "        \"Activate Generator\": {s_prime: 5 for s_prime in states}, # Generator active, meets demand\n",
        "        \"Do Not Activate Generator\": {s_prime: -5 for s_prime in states} # No generator, likely unmet demand\n",
        "    },\n",
        "    \"Medium Solar\": {\n",
        "        \"Activate Generator\": {s_prime: 5 for s_prime in states}, # Generator active, meets demand\n",
        "        \"Do Not Activate Generator\": {\n",
        "             \"Very Low Solar\": -5, \"Low Solar\": -5, \"Medium Solar\": 5, \"High Solar\": 10, \"Very High Solar\": 10\n",
        "        } # Medium might meet demand sometimes, High/Very High do\n",
        "    },\n",
        "    \"High Solar\": {\n",
        "        \"Activate Generator\": {s_prime: 5 for s_prime in states}, # Generator active, meets demand\n",
        "        \"Do Not Activate Generator\": {s_prime: 10 for s_prime in states} # High solar meets demand\n",
        "    },\n",
        "    \"Very High Solar\": {\n",
        "        \"Activate Generator\": {s_prime: 5 for s_prime in states}, # Generator active, meets demand\n",
        "        \"Do Not Activate Generator\": {s_prime: 10 for s_prime in states} # Very High solar meets demand\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define the discount factor (gamma).\n",
        "# This value (between 0 and 1) determines the present value of future rewards.\n",
        "# A higher discount factor means future rewards are considered more important.\n",
        "discount_factor = 0.9"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}